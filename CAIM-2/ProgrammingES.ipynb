{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Modifying ElasticSearch index behavior\n",
    "\n",
    "In the previous session we had to clean manually the list of words in order to compute Zipf's and Heaps' laws. \n",
    "\n",
    "ElasticSearch allows using a pipeline of processes that allows to clean the text that is indexed discarding anything not useful.\n",
    "\n",
    "We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "\n",
    "client = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Whitespace filter lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('whitespace'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 13, u'token': u'4\\xeds', u'type': u'word', u'start_offset': 10, u'position': 2}\n",
      "{u'end_offset': 18, u'token': u'was%', u'type': u'word', u'start_offset': 14, u'position': 3}\n",
      "{u'end_offset': 28, u'token': u'&printing', u'type': u'word', u'start_offset': 19, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n",
      "{u'end_offset': 46, u'token': u'the.', u'type': u'word', u'start_offset': 42, u'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'<ALPHANUM>', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'<ALPHANUM>', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 13, u'token': u'4\\xeds', u'type': u'<ALPHANUM>', u'start_offset': 10, u'position': 2}\n",
      "{u'end_offset': 17, u'token': u'was', u'type': u'<ALPHANUM>', u'start_offset': 14, u'position': 3}\n",
      "{u'end_offset': 28, u'token': u'printing', u'type': u'<ALPHANUM>', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'<ALPHANUM>', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'<ALPHANUM>', u'start_offset': 37, u'position': 6}\n",
      "{u'end_offset': 45, u'token': u'the', u'type': u'<ALPHANUM>', u'start_offset': 42, u'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 13, u'token': u'\\xeds', u'type': u'word', u'start_offset': 11, u'position': 2}\n",
      "{u'end_offset': 17, u'token': u'was', u'type': u'word', u'start_offset': 14, u'position': 3}\n",
      "{u'end_offset': 28, u'token': u'printing', u'type': u'word', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n",
      "{u'end_offset': 45, u'token': u'the', u'type': u'word', u'start_offset': 42, u'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter asciifolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 13, u'token': u'is', u'type': u'word', u'start_offset': 11, u'position': 2}\n",
      "{u'end_offset': 17, u'token': u'was', u'type': u'word', u'start_offset': 14, u'position': 3}\n",
      "{u'end_offset': 28, u'token': u'printing', u'type': u'word', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n",
      "{u'end_offset': 45, u'token': u'the', u'type': u'word', u'start_offset': 42, u'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter asciifolding + stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding', 'stop']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 28, u'token': u'printing', u'type': u'word', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'printed', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter asciifolding + stop + snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding','stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'end_offset': 2, u'token': u'my', u'type': u'word', u'start_offset': 0, u'position': 0}\n",
      "{u'end_offset': 9, u'token': u'taylor', u'type': u'word', u'start_offset': 3, u'position': 1}\n",
      "{u'end_offset': 28, u'token': u'print', u'type': u'word', u'start_offset': 20, u'position': 4}\n",
      "{u'end_offset': 36, u'token': u'print', u'type': u'word', u'start_offset': 29, u'position': 5}\n",
      "{u'end_offset': 41, u'token': u'rich', u'type': u'word', u'start_offset': 37, u'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **follow the instructions** of the documentation, index the documents from the previous session using the script 'IndexFilesPreprocess.py' and use the script 'CountWords.py' from the previous session to see how the set of tokens change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2 The index reloaded\n",
    "\n",
    "You can use the modified indexer ```IndexFilesPreprocess.py``` script to play with the different possibilities for the preprocessing pipeline.\n",
    "\n",
    "You can change the **tokenizer** and apply different processes to the tokens like lowercasing, asccii folding, removing stopwords and different stemming algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3 Computing Tf-Idf and Cosine similarity\n",
    "\n",
    "Now is your turn to work in the session task.\n",
    "\n",
    "The idea is to program a script that given two document paths obtains their ids, computes the Tf-Idf representation of the documents and then computes and prints their cosine similarity\n",
    "\n",
    "**Follow the instructions** in the documentation and and **pay attention** to the documentation that you have to deliver for this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
